{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8024cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fabs\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29d1b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = loadmat('dataset/B21.mat')\n",
    "target = loadmat('dataset/B27.mat')\n",
    "n= 640 # number of interpolated points\n",
    "source_bridge = np.zeros((2400,2,640))\n",
    "target_bridge = np.zeros((2400,2,640))\n",
    "DL_source = np.zeros(2400)\n",
    "DM_source = np.zeros(2400)\n",
    "DL_target = np.zeros(2400)\n",
    "DM_target = np.zeros(2400)\n",
    "for i in range(source['data'].shape[1]):\n",
    "    f = source['data'][0][i]\n",
    "    acc,dl,dm = interpolation(f)\n",
    "    source_bridge[i,:,:] = acc\n",
    "    DL_source[i] = dl\n",
    "    DM_source[i] = dm\n",
    "for i in range(target['data'].shape[1]):\n",
    "    f = target['data'][0][i]\n",
    "    acc,dl,dm = interpolation(f)\n",
    "    target_bridge[i,:,:] = acc\n",
    "    DL_target[i] = dl\n",
    "    DM_target[i] = dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b3fd9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boostrapping the Source and Target Bridge\n",
    "data_source_train = np.zeros((1500,2,640))\n",
    "data_source_test = np.zeros((900,2,640))\n",
    "data_target_train = np.zeros((1500,2,640))\n",
    "data_target_test = np.zeros((900,2,640))\n",
    "dl_source_train = np.zeros(1500)\n",
    "dl_source_test = np.zeros(900)\n",
    "dl_target_train = np.zeros(1500)\n",
    "dl_target_test = np.zeros(900)\n",
    "dm_source_train = np.zeros(1500)\n",
    "dm_source_test = np.zeros(900)\n",
    "dm_target_train = np.zeros(1500)\n",
    "dm_target_test = np.zeros(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc55b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    data_train,data_test,dl,dm = bootstrap(source_bridge[400*i:400*(i+1),:,:],DL_source[400*i:400*(i+1)],DM_source[400*i:400*(i+1)])\n",
    "    data_source_train[250*i:250*(i+1),:,:] = data_train\n",
    "    data_source_test[150*i:150*(i+1),:,:] = data_test\n",
    "    dl_source_train[250*i:250*(i+1)] = dl\n",
    "    dm_source_train[250*i:250*(i+1)] = dm\n",
    "    dl_source_test[150*i:150*(i+1)] = dl\n",
    "    dm_source_test[150*i:150*(i+1)] = dm\n",
    "for i in range(6):\n",
    "    data_train,data_test,dl,dm = bootstrap(target_bridge[400*i:400*(i+1),:,:],DL_target[400*i:400*(i+1)],DM_target[400*i:400*(i+1)])\n",
    "    data_target_train[250*i:250*(i+1),:,:] = data_train\n",
    "    data_target_test[150*i:150*(i+1),:,:] = data_test\n",
    "    dl_target_train[250*i:250*(i+1)] = dl\n",
    "    dm_target_train[250*i:250*(i+1)] = dm\n",
    "    dl_target_test[150*i:150*(i+1)] = dl\n",
    "    dm_target_test[150*i:150*(i+1)] = dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ecf1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Source test set: Average loss: 1.0986, 1.0987\n",
      "Target test set: Average loss: 1.0986, 1.0987\n",
      "Epoch 0\n",
      "Source test set: Average loss: 1.0981, 1.1000\n",
      "Target test set: Average loss: 1.0976, 1.1012\n",
      "Epoch 50\n",
      "Source test set: Average loss: 0.8203, 1.0370\n",
      "Target test set: Average loss: 0.7801, 0.9233\n",
      "Epoch 100\n",
      "Source test set: Average loss: 0.8039, 0.9421\n",
      "Target test set: Average loss: 0.6940, 0.8670\n",
      "Epoch 150\n",
      "Source test set: Average loss: 0.7686, 0.9184\n",
      "Target test set: Average loss: 0.6529, 0.8368\n",
      "Epoch 200\n",
      "Source test set: Average loss: 0.7471, 0.8207\n",
      "Target test set: Average loss: 0.6803, 0.8378\n",
      "Epoch 250\n",
      "Source test set: Average loss: 0.7651, 0.8518\n",
      "Target test set: Average loss: 0.6667, 0.8462\n",
      "Epoch 300\n",
      "Source test set: Average loss: 0.7505, 0.7934\n",
      "Target test set: Average loss: 0.6439, 0.8239\n"
     ]
    }
   ],
   "source": [
    "x_source_train,yl_source_train,ys_source_train=shuffle(data_source_train,dl_source_train,dm_source_train)\n",
    "x_target_train,yl_target_train,ys_target_train=shuffle(data_target_train,dl_target_train,dm_target_train)\n",
    "x_source_test,yl_source_test,ys_source_test=shuffle(data_source_test,dl_source_test,dm_source_test)\n",
    "x_target_test,yl_target_test,ys_target_test=shuffle(data_target_test,dl_target_test,dm_target_test)\n",
    "dataset_train_source = Dataset(x_source_train,yl_source_train,ys_source_train, resize_size=64, \\\n",
    "                              crop_size=62, is_train = True)\n",
    "dataset_test_source = Dataset(x_source_test,yl_source_test,ys_source_test, resize_size=64, \\\n",
    "                              crop_size=62, is_train = False)\n",
    "dataset_train_target = Dataset(x_target_train,yl_target_train,ys_target_train, resize_size=64, \\\n",
    "                              crop_size=62, is_train = True)\n",
    "dataset_test_target = Dataset(x_target_test,yl_target_test,ys_target_test, resize_size=64, \\\n",
    "                              crop_size=62, is_train = False)\n",
    "    \n",
    "batch_size = 100\n",
    "source_train = torch.utils.data.DataLoader(dataset_train_source,batch_size, shuffle=True)\n",
    "source_train_ = torch.utils.data.DataLoader(dataset_train_source,batch_size, shuffle=True)\n",
    "source_test = torch.utils.data.DataLoader(dataset_test_source,batch_size, shuffle=False)\n",
    "target_train = torch.utils.data.DataLoader(dataset_train_target,batch_size, shuffle=True)\n",
    "target_train_ = torch.utils.data.DataLoader(dataset_train_target,batch_size, shuffle=True)\n",
    "target_test = torch.utils.data.DataLoader(dataset_test_target,batch_size, shuffle=False)\n",
    "    \n",
    "device='cuda'\n",
    "model = SupDann().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr= 0.025, momentum= 0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "weight = 1.\n",
    "gamma = 1.\n",
    "allepoch=301\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "losses_save,accuracy_save,f1_save = test_st(0,200,model,source_train,source_test,\\\n",
    "                    target_train,target_test,criterion,\\\n",
    "                    criterion,criterion)\n",
    "losses = np.append(losses,losses_save)\n",
    "accuracies=np.append(accuracies,accuracy_save)\n",
    "f1s=np.append(f1s,f1_save)\n",
    "for epoch in range(allepoch):\n",
    "    len_dataloader = min(len(source_train), len(target_train))\n",
    "    total_steps = allepoch * len(source_train)\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data_source, data_source_, \\\n",
    "                    data_target, data_target_) in enumerate(zip(source_train, \\\n",
    "                    source_train_, target_train, target_train_)):\n",
    "        \n",
    "        s_img, s_label_l, s_label_s = data_source\n",
    "        s_img_, s_label_l_, s_label_s_ = data_source_\n",
    "        start_steps = epoch * len(source_train)\n",
    "        p = float(i + start_steps) / total_steps\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        optimizer = optimizer_scheduler(optimizer, p)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = len(s_label_l)\n",
    "\n",
    "        domain_labels = torch.cat((torch.zeros(batch_size),torch.ones(batch_size)))\n",
    "        domain_labels = domain_labels.long().to(device)\n",
    "\n",
    "        # source domain\n",
    "        input_s = (s_img.to(device),s_img_.to(device))\n",
    "        a,b,c,d = model(input_s,alpha)\n",
    "        d_xl_s,d_xq_s,d_xq_hier_s = b\n",
    "        latent_xl_s,latent_xq_s,latent_xq_hier_s = d\n",
    "        class_loss_l = criterion(a, s_label_l.to(device))\n",
    "        class_loss_q = criterion(c, s_label_s_.to(device))\n",
    "        t_img, t_label_l, t_label_s = data_target\n",
    "        t_img_, t_label_l_, t_label_s = data_target_\n",
    "\n",
    "        input_t = (t_img.to(device),t_img_.to(device))\n",
    "        _, bt, _,dt = model(input_t,alpha)\n",
    "        d_xl_t,d_xq_t,d_xq_hier_t = bt\n",
    "        latent_xl_t,latent_xq_t,latent_xq_hier_t = dt\n",
    "\n",
    "        class_loss = class_loss_l+class_loss_q\n",
    "        transfer_loss1 = criterion(torch.cat((d_xl_s,d_xl_t),0), domain_labels)\n",
    "        transfer_loss2 = criterion(torch.cat((d_xq_s,d_xq_t),0), domain_labels)\n",
    "        transfer_loss3 = criterion(torch.cat((d_xq_hier_s,d_xq_hier_t),0), domain_labels)\n",
    "        transfer_losses = torch.vstack((transfer_loss1,transfer_loss2))\n",
    "        transfer_loss = 0.1*torch.mul(torch.div(torch.log(torch.sum(torch.exp(torch.mul(transfer_losses,\\\n",
    "                      gamma)))),gamma),weight)+0.5*transfer_loss3\n",
    "         \n",
    "        err = class_loss+transfer_loss\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        i=i+1\n",
    "    losses_save,accuracy_save,f1_save = test_st(epoch,50,model,source_train,source_test,\\\n",
    "                    target_train,target_test,criterion,\\\n",
    "                    criterion,criterion)\n",
    "    losses = np.vstack((losses,losses_save))\n",
    "    accuracies=np.vstack((accuracies,accuracy_save))\n",
    "    f1s=np.vstack((f1s,f1_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0bb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
